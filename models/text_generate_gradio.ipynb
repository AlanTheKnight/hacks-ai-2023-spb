{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGtXkwlm4TPI"
      },
      "outputs": [],
      "source": [
        "!pip install transformers gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "qa_pipeline = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=\"AndrewChar/model-QA-5-epoch-RU\",\n",
        "    tokenizer=\"AndrewChar/model-QA-5-epoch-RU\"\n",
        ")\n",
        "gen_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model='ai-forever/rugpt3small_based_on_gpt2',\n",
        "    tokenizer='ai-forever/rugpt3small_based_on_gpt2'\n",
        ")\n",
        "\n",
        "\n",
        "def text_generation(inp):\n",
        "    inp = inp.split('_')\n",
        "    if inp[0] == \"qa\":\n",
        "        if len(inp) == 3:\n",
        "            predictions = qa_pipeline({\n",
        "                'context': inp[1],\n",
        "                'question': inp[2].split('*')[0]\n",
        "            })['answer']\n",
        "            if len(inp[2].split('*')) > 1:\n",
        "                predictions = inp[2].split('*')[1] + ' ' + predictions\n",
        "        else:\n",
        "            return f'Ожидаемая длина запроса для функции \"qa\": 3. Длина вашего запроса: {len(inp)}. Проверьте, используете ли Вы в качестве разделителя символ \"_\" и корректно ли Вы указали параметры context и question'\n",
        "    elif inp[0] == 'tg':\n",
        "        if len(inp) == 2:\n",
        "            predictions = gen_pipeline([inp[1]])[0][0]['generated_text'][len(inp[1])+1:]\n",
        "    else:\n",
        "        return 'Указанная Вами функция не поддерживается. Выберете между \"qa\" и \"tg\"'\n",
        "    return predictions\n",
        "\n",
        "demo = gr.Interface(fn=text_generation,\n",
        "             inputs=gr.inputs.Textbox(lines=5, label=\"Input Text\"),\n",
        "             outputs=gr.outputs.Textbox(label=\"Generated Text\"),\n",
        "             )\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "odA0pZ9D4XWr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}